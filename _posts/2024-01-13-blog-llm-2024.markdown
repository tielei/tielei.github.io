---
layout: post
category: "essay"
title: "2024年，大模型的哪些能力需要关注？"
date: 2024-01-13 00:00:00 +0800
published: true
---

2023是AIGC的“宇宙大爆发”之年。大模型 (LLM) 相关技术经历了飞速发展，迭代速度之快，到了令人眼花缭乱的地步。

简单总结下国内的情况：2023年上半年，算法专家们比拼的是谁先把模型训练出来；到了2023年过半的时候，人们开始追求模型的参数规模，百亿参数只能算是起步。而当人们开始着手基于大模型构建AI应用时，RAG和Agent技术便先后获得了极高的关注。

<!--more-->

时间来到2024，我们需要更理智地展望未来。大模型的能力边界在哪里？哪些能力是短期可能突破的，而哪些不能？模型能力与应用场景的结合点在哪里？这些问题至关重要，因为它决定了我们应该关注哪些东西，并投入怎样的精力。

### 大模型的技术特点

我们知道，大模型的预训练是在大量的无标注的文本语料库上完成的。使用无标注的数据进行训练，这一点非常关键。因为这就允许了训练数据集和模型本身的参数量可以scale到非常庞大的规模。

在大模型出现之前的AI技术公司，每当业务拓展到一个新的场景，都需要从头开始标注大量数据。这导致业务规模增长了，但边际成本降不下来，利润率也就上不去。所以，即使这些公司以前备受市场关注，也在技术上取得了亮眼的成绩，仍然难逃逐渐式微的命运。

大模型技术有所不同，这打破了很多人以前的认知。一旦我们得到了一个强大的经过预训练的模型，只需要针对特定场景进行少量微调，就能够完成特定任务。甚至不需要微调，只是利用few-shot能力，简单地展示几个example，也能获得不错的效果。而经过指令微调的大模型，则表现出非凡的zero-shot能力。

这催生了提示词工程 (Prompt Engineering) 的诞生和广泛使用。以前门槛很高的各种NLP task，现在统统变成了低成本的Prompt Engineering；以前一项NLP任务需要高阶的算法工程师工作个把月，现在只需一名毕业生写写prompt，一两天就能搞定。

以ChatGPT为代表的产品，在非常广泛的领域内证明了这种技术模式的有效性。你可以让ChatGPT帮你写文案，做总结，改写段落，做翻译，提取信息，写代码，等等等等。以前很难想象，这么多艰难的任务，底层都是基于同样的一套技术基础构建出来的。几乎任何问题你都可以向它提问，并得到答案。所以，基于大语言模型的to C的产品，天生就是超级App的形态。

在ChatGPT出来以后，很多人说，大部分知识工作者会被取代。因为它确实在知识的广度上表现出惊人的成绩，几乎无所不知，包罗万象。但是，大模型并不是万能的，它到底能在多大程度上取代人类？要回答这个问题，我们需要进一步拆解来看，思考大模型擅长什么，不擅长什么。这决定了我们在短中期的技术路线选择，也会指明AI创业者的机会空间在哪里。

在互联网级别的训练数据集中，大模型捕捉到了相当多的信息和模式。但大模型到底学到了哪些能力？当我们试图对这些能力进行分类时，却发现非常困难。我们都知道，在最底层，大模型是通过预测下一个token来工作的。但在如此细的粒度下，我们几乎无法进行有意义的讨论，也没有办法把能力与场景相结合。因此，我们稍微抽象一层，但不追求穷尽的描述（这过于困难）。我们接下来主要讨论其中的两大类能力：一大类是「知识」类能力；另一大类则是「格式生成」类能力。

### 关于知识

什么是知识呢？这其实又包含两种：
1. 事实性的信息。
2. 抽象类知识。自洽的、没有逻辑矛盾的一个概念体系，它基本上能够用概念的内涵和概念之间的关系所表达。

哪些属于第1种事实性的信息呢？这个比较好理解，比如历史知识或地理知识。美国独立战争是哪一年发生的？长江的发源地在哪？这些知识在大模型内部的编码，很可能更接近一种低层次的「记忆」。

而第2种抽象类知识的例子也有很多，比如，数学概念和定理，物理学定律和公式，再比如生物分子的结构和功能。这类知识似乎超出了纯粹的「记忆」能力，具备了一定的抽象层次的「理解」在里面。

这两类知识，看似相互独立，但有时候又似乎没有严格的界限。一方面，逻辑自洽与是否符合事实，可以是完全独立的两件事。大模型就很善于输出一段看似逻辑自洽的局部描述，但与真实的世界却相去甚远（所谓的「幻觉」）。另一方面，到了很多真实的业务场景中，「事实」很多时候并非纯粹客观的事实，而是人为规定的事实。比如，企业SOP基本都是人为制定的，同一类的业务流程，在不同的企业中可能是类似的，但又不完全相同。另外，人为规定的事实易变。企业内部的概念、术语、流程，既是对现实世界的一种抽象，也具备一定程度的「事实性」。

从需求角度来说，人们获取事实性的信息，通过信息检索的方式也能完成。而抽象的知识才是大模型能够体现智能的地方。但正如前面我们所分析的，这两种知识的界限有时候是模糊的，所以很难区分到底哪些问题应该基于检索，哪些问题只是求助于大模型更好。这也是现在RAG技术面临的困境。

当然，在某些简单的、限定的场景下，比如某些营销场景，要解决的问题足够明确，就是基于客户FAQ、业务流程文档做知识问答。这个时候使用所谓的「Naive RAG」也还过得去。也就是所有的query都经过召回、排序、生成这同一种处理流程。

但是，这种简单的RAG会带来一个比较严重的问题：非必要的检索过程很可能带来相关度不高的内容，从而遮蔽了大模型本来的能力，让大模型沦为一个文本润色工具。所以，未来的技术方案肯定要解决两个关键问题：
* 检索的必要性的问题。
* 如何同时利用好模型外部（检索到）的知识和模型内部的知识。

只有这样，才能提高整体系统的天花板。

### 关于深度理解

现在让我们来讨论一个有趣的问题：对于大模型更擅长的「抽象类知识」，它的天花板有多高？它有没有超过人类的潜力？

如今，大模型已经非常善于对文本段落进行总结、改写、翻译，以及完成基于常识的虚构类写作任务。这些任务需要对概念（不管是常识性的，还是专业性的）做一定程度的抽象。那么，大模型有可能像人类那样，进行更深层次的抽象吗？也就是「深度理解」？

我们看一下，人类的「深度理解」能达到什么样的程度呢？人类所期望的「深度理解」，需要在不同的概念体系之间找到逻辑关联，而这种关联既非简单的字面意义的，也不是过于抽象到仅剩哲学层面的。以计算机领域的问题为例，你可以这样问ChatGPT，动态规划和强化学习之间的区别和联系是什么？分布式一致性和事务之间的关系是什么？强化学习里的Agent概念和现在大模型技术体系里面的Agent概念，有什么区别和联系吗？ChatGPT肯定会给你一个答案，但不一定和你想要的完全一样。没有更多细节描述的支撑，我们也不知道它是真的理解了这些问题，还是说只是人云亦云地搬运了答案。

当然，这里举的这些例子，可能都是已知的问题。互联网上的资料或者书籍中，也许已经有了对于这些问题的直接讨论。大模型通过自回归的方式或许已经学到了答案的概率分布。但在真实的场景中，比如AI4Science的研究场景，再比如研发人员在调研一个复杂项目的技术方案，专利人员在评判一个专利的新颖性和创造性，作家在构思一部小说，人们期待的是大模型可以发现以前人类从未见过的关联性。

对于这些场景，我们应该抱有正确的期待。几乎可以肯定，大模型不太可能给出深思熟虑的、确定性的答案。大模型的理解深度，也大概率不会超过人类中的佼佼者。但是，相比人类，大模型有它自己的优势：它在训练阶段「阅读」了超大规模的信息，远远超过一个人类专家。它可以给出**非确定性的、但具有启发性**的思维线索。

试想，人类在面对一个未知问题时，也会大量阅读相关的资料，然后多次综合才能得到一个结论。而大模型早已把大部分资料都阅读过了。就像[一篇报道](https://mp.weixin.qq.com/s/s2L4Qn4EHOWpKq2Xlz4J3A){:target="_blank"}中所说的那样，Redis的作者把大语言模型称为一个无所不知的「白痴」。但真正学会如何利用它，可能并不是一个新手能够轻易驾驭的。毕竟，大模型并不能总是给出确定性的结果。这项技术，可能在初期更多惠及的是那些本就占据优势的人，就像Redis的作者或者数学家陶哲轩一样。

在这个方向上，我们需要做的是：
* 用广度换深度：针对垂直领域，喂给大模型更多更全的专业数据。
* 从非确定性中挖掘启发性：在产品设计上降低使用门槛，让更多人学会使用大语言模型。

### 关于格式生成和自动化

大模型生成的文本，不仅包含知识本身，还可能带有一定的格式。这个格式可以是简单的Markdown，也可以是标准的JSON字符串。更复杂的情况还包括生成代码（符合编程语言的语法，是更复杂的一种格式生成）。

总之，大模型不一定非要生成自然语言的文本，还可能生成符合某种「格式」的序列。生成自然语言文本，是为了给人看的；而生成JSON或代码，则是为了给机器读的。

一旦大模型可以生成可供机器直接读取或解析的序列，就意味着信息就从「无结构」的转成了「有结构」的。以前企业中很多不能自动化的流程，就都可以自动化了。信息一旦被转成「有结构」的，就有机会对接到下游的传统企业软件工具上，从而在更长的pipleline上实现自动化。超自动化 (Hyperautomation) 和 RPA (Robotic Process Automation) 等技术，也可能从一个小众的领域进入更广泛的视野。

符合JSON语法格式的生成能力，是从概率到确定性的桥梁。不管是信息提取，还是工具调用，都依赖JSON格式的生成。而调用工工具的能力，又是构建更复杂的Agent系统的基础。

简单总结一下，我们前面讨论了大模型相关的两大类能力：「知识」类能力和「格式生成」类能力。前者侧重内容，后者侧重形式。但就像很多其他事物一样，内容和形式永远都是不能分割的整体。只是在讨论不同问题和场景的时候，我们的侧重点有所不同罢了。

### 小结

假如是一项成熟的技术或一个技术体系，当我们想深入剖析它的时候，正确的顺序应该是，从场景出发，然后推导在这个场景下所需要的能力组成。但是，大模型的技术体系仍然在飞速发展中。就像红杉资本的一篇[报道](https://mp.weixin.qq.com/s/XXgvAyP4bRnpFoImSMdNJg){:target="_blank"}中所说，我们正处于原初之液阶段（Primordial Soup），新的产业、技术、开源生态，正在孕育之中。

大模型的很多潜力，还没有被发现和挖掘出来；人类对它的控制能力，也还有待提升。大模型的专业化和垂直化，对于超出自然语言的更广泛的知识序列的理解，也极具潜力。

智能的组成相当复杂，包括但不限于，记忆、推理、抽象、联想、信息编码和提取、使用工具。今天我们只是探讨了其中一小部分。简单概括一下，我们期待：
* 大模型与信息检索相结合，让我们随时取用现有知识。
* 大模型通过提供知识广度和启发性，来加速我们对未知领域的深度探索。
* 将浅层的信息处理自动化、规模化。

（正文完）

**其它精选文章**：

* [知识的三个层次](https://mp.weixin.qq.com/s/HnbBeQKG3SibP6q8eqVVJQ)
* [看得见的机器学习：零基础看懂神经网络](https://mp.weixin.qq.com/s/chHSDuwg20LyOcuAr26MXQ)
* [内卷、汉明问题与认知迭代](https://mp.weixin.qq.com/s/rgKkJ5wI5G5BZ6lIJZj7WA)
* [谈谈业务开发中的抽象思维](https://mp.weixin.qq.com/s/Yad53nP5uUOKXNb8ATcKBA)
* [在技术和业务中保持平衡](https://mp.weixin.qq.com/s/OUdH5RxiRyvcrFrbLOprjQ)
* [分布式领域最重要的一篇论文，到底讲了什么？](https://mp.weixin.qq.com/s/FZnJLPeTh-bV0amLO5CnoQ)
* [漫谈分布式系统、拜占庭将军问题与区块链](https://mp.weixin.qq.com/s/tngWdvoev8SQiyKt1gy5vw)
* [三个字节的历险](https://mp.weixin.qq.com/s/6Gyzfo4vF5mh59Xzvgm4UA)